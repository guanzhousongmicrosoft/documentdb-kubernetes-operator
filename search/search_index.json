{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Redirecting... If you are not redirected automatically, follow this link to v1/index.md .","title":"Home"},{"location":"multi-cloud-deployment-guide/","text":"Multi-Cloud DocumentDB Deployment Guide \u00b6 This guide provides step-by-step instructions for setting up a multi-cloud deployment of DocumentDB (see here ) using KubeFleet (see here ) to manage clusters across clouds. This setup enables high availability and disaster recovery. We assume the use of an AKS cluster and an on-prem Kubernetes cluster that have network access to one another. Other combinations are possible and will be documented as they are tested. Table of Contents \u00b6 Prerequisites Architecture Overview Setting Up the Hub Cluster Adding Clusters to Fleet Installing Operators and Dependencies Deploying DocumentDB Operator to Fleet Setting Up Replication Testing and Verification Failover Procedures Prerequisites \u00b6 Azure account Azure CLI installed and configured with appropriate permissions kubectl installed helm installed Git client MongoSH (for testing connection) Two kubernetes clusters that are network connected to each other. For example using Azure VPN Gatway Azure ExpressRoute ENV variables $AZURE_MEMBER and $ON_PREM_MEMBER with the kubectl context names for your clusters (e.g. \"azure-documentdb-cluster\", \"k3s-cluster-context\") Architecture Overview \u00b6 This multi-cloud deployment uses KubeFleet to manage DocumentDB instances across different cloud providers: Hub Cluster : Central control plane for managing all member clusters Member Clusters : Clusters in different cloud environments (Azure, On-prem) DocumentDB Operator : Custom operator for DocumentDB deployments Fleet Networking : Enables communication between clusters Setting Up the Hub Cluster \u00b6 The hub cluster serves as the central controller for managing the member clusters, find setup instructions here: https://learn.microsoft.com/en-us/azure/kubernetes-fleet/quickstart-create-fleet-and-members?tabs=without-hub-cluster Adding Clusters to Fleet \u00b6 Adding AKS cluster to the fleet \u00b6 Adding an AKS cluster to the fleet is very simple with the Azure portal: https://learn.microsoft.com/en-us/azure/kubernetes-fleet/quickstart-create-fleet-and-members-portal Adding other Cluster to Fleet \u00b6 See also the guide here: https://github.com/Azure/fleet/blob/main/docs/tutorials/Azure/JoinOnPremClustersToFleet.md # Add the hub to your kubectl config file az fleet get-credentials --resource-group fleet-resource-group --name fleet-hub-name # This needs to match the member cluster name in your kubectl config file clusterName=\"your-on-prem-cluster-name\" git clone https://github.com/kubefleet-dev/kubefleet.git cd kubefleet ./hack/membership/joinMC.sh v0.14.8 hub $clusterName cd .. Wait until the cluster shows the correct number of nodes, usually about a minute, by using the NODE-COUNT column from this command kubectl get membercluster -A Then add it to the fleet network git clone https://github.com/Azure/fleet-networking cd fleet-networking ./hack/membership/joinMC.sh v0.14.8 v0.3.8 hub $clusterName cd .. These commands also will work to add clusters from other cloud providers. Run kubectl get membercluster -A again and see True under JOINED to confirm. Installing Operators and Dependencies \u00b6 Install cert-manager on each cluster: # Install on primary kubectl config use-context $AZURE_MEMBER helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true # Install on replica kubectl config use-context $ON_PREM_MEMBER helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true # Install just the CRDs on the hub for propagation kubectl config use-context hub kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.crds.yaml Verify that cert-manager is installed correctly on each cluster: kubectl get pods -n cert-manager Output: NAMESPACE NAME READY STATUS RESTARTS cert-manager cert-manager-6795b8d569-d7lwd 1/1 Running 0 cert-manager cert-manager-cainjector-8f69cd69f7-pd9bc 1/1 Running 0 cert-manager cert-manager-webhook-7cc5dccc4b-7jmrh 1/1 Running 0 Install the DocumentDB operator on the hub: kubectl config use-context hub helm install documentdb-operator oci://ghcr.io/microsoft/documentdb-kubernetes-operator/documentdb-operator --version 0.0.1 --namespace documentdb-operator --create-namespace Verify the namespaces cnpg-system and documentdb-operator were created kubectl get namespaces Output: NAME STATUS AGE cnpg-system Active 50m ... documentdb-operator Active 50m ... Deploying DocumentDB Operator to fleet \u00b6 cat <<EOF > documentdb-base.yaml apiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: documentdb-base spec: resourceSelectors: - group: \"\" version: v1 kind: Namespace name: documentdb-operator - group: \"\" version: v1 kind: Namespace name: cnpg-system - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: documentdbs.db.microsoft.com - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: publications.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: poolers.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: clusterimagecatalogs.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: imagecatalogs.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: scheduledbackups.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: backups.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: subscriptions.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: databases.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: clusters.postgresql.cnpg.io # RBAC roles and bindings - group: \"rbac.authorization.k8s.io\" version: v1 kind: ClusterRole name: documentdb-operator-cluster-role - group: \"rbac.authorization.k8s.io\" version: v1 kind: ClusterRole name: documentdb-operator-cloudnative-pg - group: \"rbac.authorization.k8s.io\" version: v1 kind: ClusterRole name: documentdb-operator-cloudnative-pg-edit - group: \"rbac.authorization.k8s.io\" version: v1 kind: ClusterRole name: documentdb-operator-cloudnative-pg-view - group: \"rbac.authorization.k8s.io\" version: v1 kind: ClusterRoleBinding name: documentdb-operator-cluster-rolebinding - group: \"rbac.authorization.k8s.io\" version: v1 kind: ClusterRoleBinding name: documentdb-operator-cloudnative-pg - group: \"admissionregistration.k8s.io\" version: v1 kind: MutatingWebhookConfiguration name: cnpg-mutating-webhook-configuration - group: \"admissionregistration.k8s.io\" version: v1 kind: ValidatingWebhookConfiguration name: cnpg-validating-webhook-configuration policy: placementType: PickAll strategy: type: RollingUpdate EOF kubectl config use-context hub kubectl apply -f ./documentdb-base.yaml After a few seconds, ensure that the operator is running on both of the clusters kubectl config use-context $AZURE_MEMBER kubectl get deployment -n documentdb-operator kubectl config use-context $ON_PREM_MEMBER kubectl get deployment -n documentdb-operator Output: NAME READY UP-TO-DATE AVAILABLE AGE documentdb-operator 1/1 1 1 113s Setting Up Replication \u00b6 Physical replication provides high availability and disaster recovery capabilities across clusters. Create configuration maps to identify clusters: kubectl config use-context $ON_PREM_MEMBER kubectl create configmap cluster-name -n kube-system --from-literal=name=on-prem-cluster-name kubectl config use-context $AZURE_MEMBER kubectl create configmap cluster-name -n kube-system --from-literal=name=azure-cluster-name OR cat <<EOF > azure-cluster-name.yaml apiVersion: v1 kind: ConfigMap metadata: name: cluster-name namespace: kube-system data: name: \"azure-cluster-name\" EOF cat <<EOF > on-prem-name.yaml apiVersion: v1 kind: ConfigMap metadata: name: cluster-name namespace: kube-system data: name: \"on-prem-cluster-name\" EOF kubectl config use-context $AZURE_MEMBER kubectl apply -f ./primary-name.yaml kubectl config use-context $ON_PREM_MEMBER kubectl apply -f ./replica-name.yaml Apply the DocumentDB resource configuration: cat <<EOF > documentdb-resource.yaml apiVersion: v1 kind: Namespace metadata: name: documentdb-preview-ns --- apiVersion: db.microsoft.com/preview kind: DocumentDB metadata: name: documentdb-preview namespace: documentdb-preview-ns spec: nodeCount: 1 instancesPerNode: 1 documentDBImage: ghcr.io/microsoft/documentdb/documentdb-local:16 resource: pvcSize: 10Gi clusterReplication: primary: azure-cluster-name clusterList: - azure-cluster-name - on-prem-cluster-name publicLoadBalancer: enabled: true --- apiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: documentdb-crp spec: resourceSelectors: - group: \"\" version: v1 kind: Namespace name: documentdb-preview-ns policy: placementType: PickAll strategy: type: RollingUpdate EOF kubectl config use-context hub kubectl apply -f ./documentdb-resource.yaml After a few seconds, ensure that the operator is running on both of the clusters kubectl config use-context $AZURE_MEMBER kubectl get pods -n documentdb-operator-ns kubectl config use-context $ON_PREM_MEMBER kubectl get pods -n documentdb-operator-ns Output: NAME READY STATUS RESTARTS AGE azure-cluster-name-1 2/2 Running 0 3m33s Testing and Verification \u00b6 Test connection to DocumentDB: # Get the service IP from primary (azure) kubectl config use-context $AZURE_MEMBER service_ip=$(kubectl get service documentdb-service-documentdb-preview -n documentdb-preview-ns -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\") # Connect using mongosh mongosh $service_ip:10260 -u default_user -p Admin100 --authenticationMechanism SCRAM-SHA-256 --tls --tlsAllowInvalidCertificates # Create a test collection and document use test db.testCollection.insertOne({ name: \"Test Document\", value: 1 }) db.testCollection.find() For replication testing, verify data is correctly replicated: Insert data on the primary Verify it appears on the replica Failover Procedures \u00b6 Physical Replication Failover \u00b6 To initiate a failover from the primary to a replica cluster, run this against the hub: kubectl config use-context hub kubectl patch documentdb documentdb-preview -n documentdb-preview-ns \\ --type='json' -p='[ {\"op\": \"replace\", \"path\": \"/spec/clusterReplication/primary\", \"value\":\"on-prem-cluster-name\"}, {\"op\": \"replace\", \"path\": \"/spec/clusterReplication/clusterList\", \"value\":[\"on-prem-cluster-name\"]} ]' Note : This guide assumes a basic understanding of Kubernetes, container orchestration, and distributed database concepts. Adjust resource sizes, node counts, and other parameters according to your production requirements.","title":"Multi-Cloud DocumentDB Deployment Guide"},{"location":"multi-cloud-deployment-guide/#multi-cloud-documentdb-deployment-guide","text":"This guide provides step-by-step instructions for setting up a multi-cloud deployment of DocumentDB (see here ) using KubeFleet (see here ) to manage clusters across clouds. This setup enables high availability and disaster recovery. We assume the use of an AKS cluster and an on-prem Kubernetes cluster that have network access to one another. Other combinations are possible and will be documented as they are tested.","title":"Multi-Cloud DocumentDB Deployment Guide"},{"location":"multi-cloud-deployment-guide/#table-of-contents","text":"Prerequisites Architecture Overview Setting Up the Hub Cluster Adding Clusters to Fleet Installing Operators and Dependencies Deploying DocumentDB Operator to Fleet Setting Up Replication Testing and Verification Failover Procedures","title":"Table of Contents"},{"location":"multi-cloud-deployment-guide/#prerequisites","text":"Azure account Azure CLI installed and configured with appropriate permissions kubectl installed helm installed Git client MongoSH (for testing connection) Two kubernetes clusters that are network connected to each other. For example using Azure VPN Gatway Azure ExpressRoute ENV variables $AZURE_MEMBER and $ON_PREM_MEMBER with the kubectl context names for your clusters (e.g. \"azure-documentdb-cluster\", \"k3s-cluster-context\")","title":"Prerequisites"},{"location":"multi-cloud-deployment-guide/#architecture-overview","text":"This multi-cloud deployment uses KubeFleet to manage DocumentDB instances across different cloud providers: Hub Cluster : Central control plane for managing all member clusters Member Clusters : Clusters in different cloud environments (Azure, On-prem) DocumentDB Operator : Custom operator for DocumentDB deployments Fleet Networking : Enables communication between clusters","title":"Architecture Overview"},{"location":"multi-cloud-deployment-guide/#setting-up-the-hub-cluster","text":"The hub cluster serves as the central controller for managing the member clusters, find setup instructions here: https://learn.microsoft.com/en-us/azure/kubernetes-fleet/quickstart-create-fleet-and-members?tabs=without-hub-cluster","title":"Setting Up the Hub Cluster"},{"location":"multi-cloud-deployment-guide/#adding-clusters-to-fleet","text":"","title":"Adding Clusters to Fleet"},{"location":"multi-cloud-deployment-guide/#adding-aks-cluster-to-the-fleet","text":"Adding an AKS cluster to the fleet is very simple with the Azure portal: https://learn.microsoft.com/en-us/azure/kubernetes-fleet/quickstart-create-fleet-and-members-portal","title":"Adding AKS cluster to the fleet"},{"location":"multi-cloud-deployment-guide/#adding-other-cluster-to-fleet","text":"See also the guide here: https://github.com/Azure/fleet/blob/main/docs/tutorials/Azure/JoinOnPremClustersToFleet.md # Add the hub to your kubectl config file az fleet get-credentials --resource-group fleet-resource-group --name fleet-hub-name # This needs to match the member cluster name in your kubectl config file clusterName=\"your-on-prem-cluster-name\" git clone https://github.com/kubefleet-dev/kubefleet.git cd kubefleet ./hack/membership/joinMC.sh v0.14.8 hub $clusterName cd .. Wait until the cluster shows the correct number of nodes, usually about a minute, by using the NODE-COUNT column from this command kubectl get membercluster -A Then add it to the fleet network git clone https://github.com/Azure/fleet-networking cd fleet-networking ./hack/membership/joinMC.sh v0.14.8 v0.3.8 hub $clusterName cd .. These commands also will work to add clusters from other cloud providers. Run kubectl get membercluster -A again and see True under JOINED to confirm.","title":"Adding other Cluster to Fleet"},{"location":"multi-cloud-deployment-guide/#installing-operators-and-dependencies","text":"Install cert-manager on each cluster: # Install on primary kubectl config use-context $AZURE_MEMBER helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true # Install on replica kubectl config use-context $ON_PREM_MEMBER helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true # Install just the CRDs on the hub for propagation kubectl config use-context hub kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.crds.yaml Verify that cert-manager is installed correctly on each cluster: kubectl get pods -n cert-manager Output: NAMESPACE NAME READY STATUS RESTARTS cert-manager cert-manager-6795b8d569-d7lwd 1/1 Running 0 cert-manager cert-manager-cainjector-8f69cd69f7-pd9bc 1/1 Running 0 cert-manager cert-manager-webhook-7cc5dccc4b-7jmrh 1/1 Running 0 Install the DocumentDB operator on the hub: kubectl config use-context hub helm install documentdb-operator oci://ghcr.io/microsoft/documentdb-kubernetes-operator/documentdb-operator --version 0.0.1 --namespace documentdb-operator --create-namespace Verify the namespaces cnpg-system and documentdb-operator were created kubectl get namespaces Output: NAME STATUS AGE cnpg-system Active 50m ... documentdb-operator Active 50m ...","title":"Installing Operators and Dependencies"},{"location":"multi-cloud-deployment-guide/#deploying-documentdb-operator-to-fleet","text":"cat <<EOF > documentdb-base.yaml apiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: documentdb-base spec: resourceSelectors: - group: \"\" version: v1 kind: Namespace name: documentdb-operator - group: \"\" version: v1 kind: Namespace name: cnpg-system - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: documentdbs.db.microsoft.com - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: publications.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: poolers.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: clusterimagecatalogs.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: imagecatalogs.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: scheduledbackups.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: backups.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: subscriptions.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: databases.postgresql.cnpg.io - group: \"apiextensions.k8s.io\" version: v1 kind: CustomResourceDefinition name: clusters.postgresql.cnpg.io # RBAC roles and bindings - group: \"rbac.authorization.k8s.io\" version: v1 kind: ClusterRole name: documentdb-operator-cluster-role - group: \"rbac.authorization.k8s.io\" version: v1 kind: ClusterRole name: documentdb-operator-cloudnative-pg - group: \"rbac.authorization.k8s.io\" version: v1 kind: ClusterRole name: documentdb-operator-cloudnative-pg-edit - group: \"rbac.authorization.k8s.io\" version: v1 kind: ClusterRole name: documentdb-operator-cloudnative-pg-view - group: \"rbac.authorization.k8s.io\" version: v1 kind: ClusterRoleBinding name: documentdb-operator-cluster-rolebinding - group: \"rbac.authorization.k8s.io\" version: v1 kind: ClusterRoleBinding name: documentdb-operator-cloudnative-pg - group: \"admissionregistration.k8s.io\" version: v1 kind: MutatingWebhookConfiguration name: cnpg-mutating-webhook-configuration - group: \"admissionregistration.k8s.io\" version: v1 kind: ValidatingWebhookConfiguration name: cnpg-validating-webhook-configuration policy: placementType: PickAll strategy: type: RollingUpdate EOF kubectl config use-context hub kubectl apply -f ./documentdb-base.yaml After a few seconds, ensure that the operator is running on both of the clusters kubectl config use-context $AZURE_MEMBER kubectl get deployment -n documentdb-operator kubectl config use-context $ON_PREM_MEMBER kubectl get deployment -n documentdb-operator Output: NAME READY UP-TO-DATE AVAILABLE AGE documentdb-operator 1/1 1 1 113s","title":"Deploying DocumentDB Operator to fleet"},{"location":"multi-cloud-deployment-guide/#setting-up-replication","text":"Physical replication provides high availability and disaster recovery capabilities across clusters. Create configuration maps to identify clusters: kubectl config use-context $ON_PREM_MEMBER kubectl create configmap cluster-name -n kube-system --from-literal=name=on-prem-cluster-name kubectl config use-context $AZURE_MEMBER kubectl create configmap cluster-name -n kube-system --from-literal=name=azure-cluster-name OR cat <<EOF > azure-cluster-name.yaml apiVersion: v1 kind: ConfigMap metadata: name: cluster-name namespace: kube-system data: name: \"azure-cluster-name\" EOF cat <<EOF > on-prem-name.yaml apiVersion: v1 kind: ConfigMap metadata: name: cluster-name namespace: kube-system data: name: \"on-prem-cluster-name\" EOF kubectl config use-context $AZURE_MEMBER kubectl apply -f ./primary-name.yaml kubectl config use-context $ON_PREM_MEMBER kubectl apply -f ./replica-name.yaml Apply the DocumentDB resource configuration: cat <<EOF > documentdb-resource.yaml apiVersion: v1 kind: Namespace metadata: name: documentdb-preview-ns --- apiVersion: db.microsoft.com/preview kind: DocumentDB metadata: name: documentdb-preview namespace: documentdb-preview-ns spec: nodeCount: 1 instancesPerNode: 1 documentDBImage: ghcr.io/microsoft/documentdb/documentdb-local:16 resource: pvcSize: 10Gi clusterReplication: primary: azure-cluster-name clusterList: - azure-cluster-name - on-prem-cluster-name publicLoadBalancer: enabled: true --- apiVersion: placement.kubernetes-fleet.io/v1beta1 kind: ClusterResourcePlacement metadata: name: documentdb-crp spec: resourceSelectors: - group: \"\" version: v1 kind: Namespace name: documentdb-preview-ns policy: placementType: PickAll strategy: type: RollingUpdate EOF kubectl config use-context hub kubectl apply -f ./documentdb-resource.yaml After a few seconds, ensure that the operator is running on both of the clusters kubectl config use-context $AZURE_MEMBER kubectl get pods -n documentdb-operator-ns kubectl config use-context $ON_PREM_MEMBER kubectl get pods -n documentdb-operator-ns Output: NAME READY STATUS RESTARTS AGE azure-cluster-name-1 2/2 Running 0 3m33s","title":"Setting Up Replication"},{"location":"multi-cloud-deployment-guide/#testing-and-verification","text":"Test connection to DocumentDB: # Get the service IP from primary (azure) kubectl config use-context $AZURE_MEMBER service_ip=$(kubectl get service documentdb-service-documentdb-preview -n documentdb-preview-ns -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\") # Connect using mongosh mongosh $service_ip:10260 -u default_user -p Admin100 --authenticationMechanism SCRAM-SHA-256 --tls --tlsAllowInvalidCertificates # Create a test collection and document use test db.testCollection.insertOne({ name: \"Test Document\", value: 1 }) db.testCollection.find() For replication testing, verify data is correctly replicated: Insert data on the primary Verify it appears on the replica","title":"Testing and Verification"},{"location":"multi-cloud-deployment-guide/#failover-procedures","text":"","title":"Failover Procedures"},{"location":"multi-cloud-deployment-guide/#physical-replication-failover","text":"To initiate a failover from the primary to a replica cluster, run this against the hub: kubectl config use-context hub kubectl patch documentdb documentdb-preview -n documentdb-preview-ns \\ --type='json' -p='[ {\"op\": \"replace\", \"path\": \"/spec/clusterReplication/primary\", \"value\":\"on-prem-cluster-name\"}, {\"op\": \"replace\", \"path\": \"/spec/clusterReplication/clusterList\", \"value\":[\"on-prem-cluster-name\"]} ]' Note : This guide assumes a basic understanding of Kubernetes, container orchestration, and distributed database concepts. Adjust resource sizes, node counts, and other parameters according to your production requirements.","title":"Physical Replication Failover"},{"location":"v1/","text":"DocumentDB Kubernetes Operator \u00b6 The DocumentDB Kubernetes Operator is an open-source project to run and manage DocumentDB on Kubernetes. DocumentDB is the engine powering vCore-based Azure Cosmos DB for MongoDB. It is built on top of PostgreSQL and offers a native implementation of document-oriented NoSQL database, enabling CRUD operations on BSON data types. As part of a DocumentDB cluster installation, the operator deploys and manages a set of PostgreSQL instance(s), the DocumentDB Gateway , as well as other Kubernetes resources. While PostgreSQL is used as the underlying storage engine, the gateway ensures that you can connect to the DocumentDB cluster using MongoDB-compatible drivers, APIs, and tools. Note: This project is under active development but not yet recommended for production use. We welcome your feedback and contributions! Quickstart \u00b6 This quickstart guide will walk you through the steps to install the operator, deploy a DocumentDB cluster, access it using mongosh , and perform basic operations. Prerequisites \u00b6 Helm installed. kubectl installed. A local Kubernetes cluster such as minikube , or kind installed. You are free to use any other Kubernetes cluster, but that's not a requirement for this quickstart. Install mongosh to connect to the DocumentDB cluster. Start a local Kubernetes cluster \u00b6 If you are using minikube , use the following command: minikube start If you are using kind , use the following command: kind create cluster Install cert-manager \u00b6 cert-manager is used to manage TLS certificates for the DocumentDB cluster. If you already have cert-manager installed, you can skip this step. helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true Verify that cert-manager is installed correctly: kubectl get pods -n cert-manager Output: NAMESPACE NAME READY STATUS RESTARTS cert-manager cert-manager-6794b8d569-d7lwd 1/1 Running 0 cert-manager cert-manager-cainjector-7f69cd69f7-pd9bc 1/1 Running 0 cert-manager cert-manager-webhook-6cc5dccc4b-7jmrh 1/1 Running 0 Install documentdb-operator using the Helm chart \u00b6 The DocumentDB operator utilizes the CloudNativePG operator behind the scenes, and installs it in the cnpg-system namespace. At this point, it is assumed that the CloudNativePG operator is not pre-installed in your cluster. Use the following command to install the DocumentDB operator: helm install documentdb-operator oci://ghcr.io/microsoft/documentdb-kubernetes-operator/documentdb-operator --version 0.0.1 --namespace documentdb-operator --create-namespace This will install the operator in the documentdb-operator namespace. Verify that it is running: kubectl get deployment -n documentdb-operator Output: NAME READY UP-TO-DATE AVAILABLE AGE documentdb-operator 1/1 1 1 113s You should also see the DocumentDB operator CRDs installed in the cluster: kubectl get crd | grep documentdb Output: documentdbs.db.microsoft.com Deploy a DocumentDB cluster \u00b6 Create a single-node DocumentDB cluster: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: documentdb-preview-ns --- apiVersion: db.microsoft.com/preview kind: DocumentDB metadata: name: documentdb-preview namespace: documentdb-preview-ns spec: nodeCount: 1 instancesPerNode: 1 documentDBImage: ghcr.io/microsoft/documentdb/documentdb-local:16 resource: pvcSize: 10Gi publicLoadBalancer: enabled: false EOF Wait for the DocumentDB cluster to be fully initialized. Verify that it is running: kubectl get pods -n documentdb-preview-ns Output: NAME READY STATUS RESTARTS AGE documentdb-preview-1 2/2 Running 0 26m You can also check the DocumentDB CRD instance: kubectl get DocumentDB -n documentdb-preview-ns Output: NAME AGE documentdb-preview 28m Connect to the DocumentDB cluster \u00b6 The DocumentDB Pod has the Gateway container running as a sidecar. To keep things simple, the quickstart does not use a public load balancer. So you can connect to the DocumentDB instance directly through the Gateway port 10260 . For both minikube and kind , this can be easily done using port forwarding: kubectl port-forward pod/documentdb-preview-1 10260:10260 -n documentdb-preview-ns Connect using mongosh : mongosh 127.0.0.1:10260 -u default_user -p Admin100 --authenticationMechanism SCRAM-SHA-256 --tls --tlsAllowInvalidCertificates Execute the following commands to create a database and a collection, and insert some documents: use testdb db.createCollection(\"test_collection\") db.test_collection.insertMany([ { name: \"Alice\", age: 30 }, { name: \"Bob\", age: 25 }, { name: \"Charlie\", age: 35 } ]) db.test_collection.find() Output: [direct: mongos] test> use testdb switched to db testdb [direct: mongos] testdb> db.createCollection(\"test_collection\") { ok: 1 } [direct: mongos] testdb> db.test_collection.insertMany([ ... { name: \"Alice\", age: 30 }, ... { name: \"Bob\", age: 25 }, ... { name: \"Charlie\", age: 35 } ... ]) { acknowledged: true, insertedIds: { '0': ObjectId('682c3b06491dc99ae02b3fed'), '1': ObjectId('682c3b06491dc99ae02b3fee'), '2': ObjectId('682c3b06491dc99ae02b3fef') } } [direct: mongos] testdb> db.test_collection.find() [ { _id: ObjectId('682c3b06491dc99ae02b3fed'), name: 'Alice', age: 30 }, { _id: ObjectId('682c3b06491dc99ae02b3fee'), name: 'Bob', age: 25 }, { _id: ObjectId('682c3b06491dc99ae02b3fef'), name: 'Charlie', age: 35 } ] Other options: Try the sample Python app and LoadBalancer service \u00b6 Connect to DocumentDB using a Python app \u00b6 In addition to mongosh , you can also use the sample Python program (that uses the PyMongo client) in the GitHub repository to execute operations on the DocumentDB instance. It inserts a sample document to a movies collection inside the sample_mflix database. git clone https://github.com/microsoft/documentdb-kubernetes-operator cd documentdb-kubernetes-operator/scripts/test-scripts pip3 install pymongo python3 mongo-python-data-pusher.py Output: Inserted document ID: 682c54f9505b85fba77ed154 {'_id': ObjectId('682c54f9505b85fba77ed154'), 'cast': ['Olivia Colman', 'Emma Stone', 'Rachel Weisz'], 'directors': ['Yorgos Lanthimos'], 'genres': ['Drama', 'History'], 'rated': 'R', 'runtime': 121, 'title': 'The Favourite MongoDB Movie', 'type': 'movie', 'year': 2018} You can verify this using the mongosh shell: use sample_mflix db.movies.find() Output: [direct: mongos] testdb> use sample_mflix switched to db sample_mflix [direct: mongos] sample_mflix> [direct: mongos] sample_mflix> db.movies.find() [ { _id: ObjectId('682c54f9505b85fba77ed154'), title: 'The Favourite MongoDB Movie', genres: [ 'Drama', 'History' ], runtime: 121, rated: 'R', year: 2018, directors: [ 'Yorgos Lanthimos' ], cast: [ 'Olivia Colman', 'Emma Stone', 'Rachel Weisz' ], type: 'movie' } ] Use a LoadBalancer service \u00b6 For the quickstart, you connected to DocumentDB using port forwarding. If you are using a Kubernetes cluster in the cloud (for example, Azure Kubernetes Service ), and want to use a LoadBalancer service instead, enable it in the DocumentDB spec as follows: publicLoadBalancer: enabled: true LoadBalancer service is also supported in minikube and kind . List the Service s and verify: kubectl get services -n documentdb-preview-ns This will create a LoadBalancer service named documentdb-service-documentdb-preview for the DocumentDB cluster. You can then access the DocumentDB instance using the external IP of the service. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE documentdb-preview-r ClusterIP 10.0.216.38 <none> 5432/TCP 26m documentdb-preview-ro ClusterIP 10.0.31.103 <none> 5432/TCP 26m documentdb-preview-rw ClusterIP 10.0.118.26 <none> 5432/TCP 26m documentdb-service-documentdb-preview LoadBalancer 10.0.228.243 52.149.56.216 10260:30312/TCP 27m If you are using the Python program to connect to DocumentDB, make sure to update the script's host variable with the external IP of your documentdb-service-documentdb-preview LoadBalancer service. Additionally, ensure that you update the default password in the script or, preferably, use environment variables to securely manage sensitive information like passwords. Delete the DocumentDB cluster and other resources \u00b6 kubectl delete DocumentDB documentdb-preview -n documentdb-preview-ns The Pod should now be terminated: kubectl get pods -n documentdb-preview-ns Uninstall the DocumentDB operator: helm uninstall documentdb-operator --namespace documentdb-operator Output: These resources were kept due to the resource policy: [CustomResourceDefinition] poolers.postgresql.cnpg.io [CustomResourceDefinition] publications.postgresql.cnpg.io [CustomResourceDefinition] scheduledbackups.postgresql.cnpg.io [CustomResourceDefinition] subscriptions.postgresql.cnpg.io [CustomResourceDefinition] backups.postgresql.cnpg.io [CustomResourceDefinition] clusterimagecatalogs.postgresql.cnpg.io [CustomResourceDefinition] clusters.postgresql.cnpg.io [CustomResourceDefinition] databases.postgresql.cnpg.io [CustomResourceDefinition] imagecatalogs.postgresql.cnpg.io release \"documentdb-operator\" uninstalled Verify that the Pod is removed: kubectl get pods -n documentdb-preview-ns Delete namespace, and CRDs: kubectl delete namespace documentdb-operator kubectl delete crd backups.postgresql.cnpg.io \\ clusterimagecatalogs.postgresql.cnpg.io \\ clusters.postgresql.cnpg.io \\ databases.postgresql.cnpg.io \\ imagecatalogs.postgresql.cnpg.io \\ poolers.postgresql.cnpg.io \\ publications.postgresql.cnpg.io \\ scheduledbackups.postgresql.cnpg.io \\ subscriptions.postgresql.cnpg.io \\ documentdbs.db.microsoft.com","title":"Get Started"},{"location":"v1/#documentdb-kubernetes-operator","text":"The DocumentDB Kubernetes Operator is an open-source project to run and manage DocumentDB on Kubernetes. DocumentDB is the engine powering vCore-based Azure Cosmos DB for MongoDB. It is built on top of PostgreSQL and offers a native implementation of document-oriented NoSQL database, enabling CRUD operations on BSON data types. As part of a DocumentDB cluster installation, the operator deploys and manages a set of PostgreSQL instance(s), the DocumentDB Gateway , as well as other Kubernetes resources. While PostgreSQL is used as the underlying storage engine, the gateway ensures that you can connect to the DocumentDB cluster using MongoDB-compatible drivers, APIs, and tools. Note: This project is under active development but not yet recommended for production use. We welcome your feedback and contributions!","title":"DocumentDB Kubernetes Operator"},{"location":"v1/#quickstart","text":"This quickstart guide will walk you through the steps to install the operator, deploy a DocumentDB cluster, access it using mongosh , and perform basic operations.","title":"Quickstart"},{"location":"v1/#prerequisites","text":"Helm installed. kubectl installed. A local Kubernetes cluster such as minikube , or kind installed. You are free to use any other Kubernetes cluster, but that's not a requirement for this quickstart. Install mongosh to connect to the DocumentDB cluster.","title":"Prerequisites"},{"location":"v1/#start-a-local-kubernetes-cluster","text":"If you are using minikube , use the following command: minikube start If you are using kind , use the following command: kind create cluster","title":"Start a local Kubernetes cluster"},{"location":"v1/#install-cert-manager","text":"cert-manager is used to manage TLS certificates for the DocumentDB cluster. If you already have cert-manager installed, you can skip this step. helm repo add jetstack https://charts.jetstack.io helm repo update helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true Verify that cert-manager is installed correctly: kubectl get pods -n cert-manager Output: NAMESPACE NAME READY STATUS RESTARTS cert-manager cert-manager-6794b8d569-d7lwd 1/1 Running 0 cert-manager cert-manager-cainjector-7f69cd69f7-pd9bc 1/1 Running 0 cert-manager cert-manager-webhook-6cc5dccc4b-7jmrh 1/1 Running 0","title":"Install cert-manager"},{"location":"v1/#install-documentdb-operator-using-the-helm-chart","text":"The DocumentDB operator utilizes the CloudNativePG operator behind the scenes, and installs it in the cnpg-system namespace. At this point, it is assumed that the CloudNativePG operator is not pre-installed in your cluster. Use the following command to install the DocumentDB operator: helm install documentdb-operator oci://ghcr.io/microsoft/documentdb-kubernetes-operator/documentdb-operator --version 0.0.1 --namespace documentdb-operator --create-namespace This will install the operator in the documentdb-operator namespace. Verify that it is running: kubectl get deployment -n documentdb-operator Output: NAME READY UP-TO-DATE AVAILABLE AGE documentdb-operator 1/1 1 1 113s You should also see the DocumentDB operator CRDs installed in the cluster: kubectl get crd | grep documentdb Output: documentdbs.db.microsoft.com","title":"Install documentdb-operator using the Helm chart"},{"location":"v1/#deploy-a-documentdb-cluster","text":"Create a single-node DocumentDB cluster: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: documentdb-preview-ns --- apiVersion: db.microsoft.com/preview kind: DocumentDB metadata: name: documentdb-preview namespace: documentdb-preview-ns spec: nodeCount: 1 instancesPerNode: 1 documentDBImage: ghcr.io/microsoft/documentdb/documentdb-local:16 resource: pvcSize: 10Gi publicLoadBalancer: enabled: false EOF Wait for the DocumentDB cluster to be fully initialized. Verify that it is running: kubectl get pods -n documentdb-preview-ns Output: NAME READY STATUS RESTARTS AGE documentdb-preview-1 2/2 Running 0 26m You can also check the DocumentDB CRD instance: kubectl get DocumentDB -n documentdb-preview-ns Output: NAME AGE documentdb-preview 28m","title":"Deploy a DocumentDB cluster"},{"location":"v1/#connect-to-the-documentdb-cluster","text":"The DocumentDB Pod has the Gateway container running as a sidecar. To keep things simple, the quickstart does not use a public load balancer. So you can connect to the DocumentDB instance directly through the Gateway port 10260 . For both minikube and kind , this can be easily done using port forwarding: kubectl port-forward pod/documentdb-preview-1 10260:10260 -n documentdb-preview-ns Connect using mongosh : mongosh 127.0.0.1:10260 -u default_user -p Admin100 --authenticationMechanism SCRAM-SHA-256 --tls --tlsAllowInvalidCertificates Execute the following commands to create a database and a collection, and insert some documents: use testdb db.createCollection(\"test_collection\") db.test_collection.insertMany([ { name: \"Alice\", age: 30 }, { name: \"Bob\", age: 25 }, { name: \"Charlie\", age: 35 } ]) db.test_collection.find() Output: [direct: mongos] test> use testdb switched to db testdb [direct: mongos] testdb> db.createCollection(\"test_collection\") { ok: 1 } [direct: mongos] testdb> db.test_collection.insertMany([ ... { name: \"Alice\", age: 30 }, ... { name: \"Bob\", age: 25 }, ... { name: \"Charlie\", age: 35 } ... ]) { acknowledged: true, insertedIds: { '0': ObjectId('682c3b06491dc99ae02b3fed'), '1': ObjectId('682c3b06491dc99ae02b3fee'), '2': ObjectId('682c3b06491dc99ae02b3fef') } } [direct: mongos] testdb> db.test_collection.find() [ { _id: ObjectId('682c3b06491dc99ae02b3fed'), name: 'Alice', age: 30 }, { _id: ObjectId('682c3b06491dc99ae02b3fee'), name: 'Bob', age: 25 }, { _id: ObjectId('682c3b06491dc99ae02b3fef'), name: 'Charlie', age: 35 } ]","title":"Connect to the DocumentDB cluster"},{"location":"v1/#other-options-try-the-sample-python-app-and-loadbalancer-service","text":"","title":"Other options: Try the sample Python app and LoadBalancer service"},{"location":"v1/#connect-to-documentdb-using-a-python-app","text":"In addition to mongosh , you can also use the sample Python program (that uses the PyMongo client) in the GitHub repository to execute operations on the DocumentDB instance. It inserts a sample document to a movies collection inside the sample_mflix database. git clone https://github.com/microsoft/documentdb-kubernetes-operator cd documentdb-kubernetes-operator/scripts/test-scripts pip3 install pymongo python3 mongo-python-data-pusher.py Output: Inserted document ID: 682c54f9505b85fba77ed154 {'_id': ObjectId('682c54f9505b85fba77ed154'), 'cast': ['Olivia Colman', 'Emma Stone', 'Rachel Weisz'], 'directors': ['Yorgos Lanthimos'], 'genres': ['Drama', 'History'], 'rated': 'R', 'runtime': 121, 'title': 'The Favourite MongoDB Movie', 'type': 'movie', 'year': 2018} You can verify this using the mongosh shell: use sample_mflix db.movies.find() Output: [direct: mongos] testdb> use sample_mflix switched to db sample_mflix [direct: mongos] sample_mflix> [direct: mongos] sample_mflix> db.movies.find() [ { _id: ObjectId('682c54f9505b85fba77ed154'), title: 'The Favourite MongoDB Movie', genres: [ 'Drama', 'History' ], runtime: 121, rated: 'R', year: 2018, directors: [ 'Yorgos Lanthimos' ], cast: [ 'Olivia Colman', 'Emma Stone', 'Rachel Weisz' ], type: 'movie' } ]","title":"Connect to DocumentDB using a Python app"},{"location":"v1/#use-a-loadbalancer-service","text":"For the quickstart, you connected to DocumentDB using port forwarding. If you are using a Kubernetes cluster in the cloud (for example, Azure Kubernetes Service ), and want to use a LoadBalancer service instead, enable it in the DocumentDB spec as follows: publicLoadBalancer: enabled: true LoadBalancer service is also supported in minikube and kind . List the Service s and verify: kubectl get services -n documentdb-preview-ns This will create a LoadBalancer service named documentdb-service-documentdb-preview for the DocumentDB cluster. You can then access the DocumentDB instance using the external IP of the service. NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE documentdb-preview-r ClusterIP 10.0.216.38 <none> 5432/TCP 26m documentdb-preview-ro ClusterIP 10.0.31.103 <none> 5432/TCP 26m documentdb-preview-rw ClusterIP 10.0.118.26 <none> 5432/TCP 26m documentdb-service-documentdb-preview LoadBalancer 10.0.228.243 52.149.56.216 10260:30312/TCP 27m If you are using the Python program to connect to DocumentDB, make sure to update the script's host variable with the external IP of your documentdb-service-documentdb-preview LoadBalancer service. Additionally, ensure that you update the default password in the script or, preferably, use environment variables to securely manage sensitive information like passwords.","title":"Use a LoadBalancer service"},{"location":"v1/#delete-the-documentdb-cluster-and-other-resources","text":"kubectl delete DocumentDB documentdb-preview -n documentdb-preview-ns The Pod should now be terminated: kubectl get pods -n documentdb-preview-ns Uninstall the DocumentDB operator: helm uninstall documentdb-operator --namespace documentdb-operator Output: These resources were kept due to the resource policy: [CustomResourceDefinition] poolers.postgresql.cnpg.io [CustomResourceDefinition] publications.postgresql.cnpg.io [CustomResourceDefinition] scheduledbackups.postgresql.cnpg.io [CustomResourceDefinition] subscriptions.postgresql.cnpg.io [CustomResourceDefinition] backups.postgresql.cnpg.io [CustomResourceDefinition] clusterimagecatalogs.postgresql.cnpg.io [CustomResourceDefinition] clusters.postgresql.cnpg.io [CustomResourceDefinition] databases.postgresql.cnpg.io [CustomResourceDefinition] imagecatalogs.postgresql.cnpg.io release \"documentdb-operator\" uninstalled Verify that the Pod is removed: kubectl get pods -n documentdb-preview-ns Delete namespace, and CRDs: kubectl delete namespace documentdb-operator kubectl delete crd backups.postgresql.cnpg.io \\ clusterimagecatalogs.postgresql.cnpg.io \\ clusters.postgresql.cnpg.io \\ databases.postgresql.cnpg.io \\ imagecatalogs.postgresql.cnpg.io \\ poolers.postgresql.cnpg.io \\ publications.postgresql.cnpg.io \\ scheduledbackups.postgresql.cnpg.io \\ subscriptions.postgresql.cnpg.io \\ documentdbs.db.microsoft.com","title":"Delete the DocumentDB cluster and other resources"}]}